<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Point Estimate - Omanshu&#39;s blog</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="Point Estimate" />
<meta property="og:description" content="This is going to take some time and effort to read, as it took to write; but I’ll try to answer to my best knowledge. I will follow a standard approach, but any terms that require some prior knowledge would contain wiki links.
##Background
MAP (maximum a priori estimate), MLE (maximum likelihood estimate) and MoM (method of moments) in this context refer to point estimation problems and are among many other estimation methods in statistics." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://omanshuthapliyal.github.io/blog/blog/posts/pointestimate/" />
<meta property="article:published_time" content="2019-08-02T07:58:15-04:00"/>
<meta property="article:modified_time" content="2019-08-02T07:58:15-04:00"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Point Estimate"/>
<meta name="twitter:description" content="This is going to take some time and effort to read, as it took to write; but I’ll try to answer to my best knowledge. I will follow a standard approach, but any terms that require some prior knowledge would contain wiki links.
##Background
MAP (maximum a priori estimate), MLE (maximum likelihood estimate) and MoM (method of moments) in this context refer to point estimation problems and are among many other estimation methods in statistics."/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:500,100,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="../../../css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="../../../css/main.css" /><script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script><script src="../../../js/main.js"></script>
</head>

<body>
	<div class="container wrapper post">
		<div class="header">
	<h1 class="site-title">Omanshu&#39;s blog</h1>
	<div class="site-description"><h2>I will post here about many things from a bird&rsquo;s eye view until I think I know enough to write about a few in some depth.</h2><nav class="nav social">
			<ul class="flat"><a href="https://github.com/omanshuthapliyal" title="Github"><i data-feather="github"></i></a><a href="https://www.linkedin.com/in/omanshu/" title="Linkedin"><i data-feather="linkedin"></i></a><a href="https://www.facebook.com/omanshu" title="Facebook"><i data-feather="facebook"></i></a></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="http://omanshuthapliyal.github.io/">Home</a>
			</li>
			
			<li>
				<a href="../posts/">All posts</a>
			</li>
			
			<li>
				<a href="../tags/">Tags</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post-header">
			<h1 class="title">Point Estimate</h1>
			<div class="meta">Posted at &mdash; Aug 2, 2019</div>
		</div>

		<div class="markdown">
			<p>This is going to take some time and effort to read, as it took to write; but I’ll try to answer to my best knowledge. I will follow a standard approach, but any terms that require some prior knowledge would contain wiki links.</p>

<p>##Background</p>

<p>MAP (maximum a priori estimate), MLE (maximum likelihood estimate) and MoM (method of moments) in this context refer to <a href="https://en.wikipedia.org/wiki/Point_estimation">point estimation</a> problems and are among many other estimation methods in statistics.</p>

<p>##Point estimation problem</p>

<p>A generic point estimation problem goes like this:
Consider a random variable <span  class="math">\(x\sim f(x;\theta)\)</span> with the parameter <span  class="math">\(\theta\in\Omega\)</span>. Here <span  class="math">\(f(x;\theta)\)</span> corresponds to a <em>family of distributions</em> rather than a single probability distribution. We are interested in finding a <em>point</em> estimate to the parameter <span  class="math">\(\theta\)</span>. In order to do that, we draw a random sample with <span  class="math">\(n\)</span> realizations of the random variable <span  class="math">\(x\)</span> as <span  class="math">\(\tilde{X}=\{x_1=X_1,x_2=X_2,\cdots, x_n=X_n\}\)</span>. In shorthand, this realization from the <span  class="math">\(n\)</span> experiments of the given family of distribution is also written as simply <span  class="math">\(\{X_1,X_2,\cdots,X_n\}\)</span>.</p>

<p>##Solution strategies</p>

<p>Our point estimate, in order to capture the information in the realization <span  class="math">\(\tilde{X}\)</span>, has to depend on another statistic <span  class="math">\(Y=u(\tilde{X})\)</span>, such that our point estimate is:
$\hat{\theta}=u(\tilde{X})$$
This is a key point and is intuitive to the problem of point estimation.</p>

<p>##Maximum likelihood</p>

<p>One of the ways to go about finding $\hat{\theta}$ is to find *what value of parameter *<span  class="math">\(\theta\)</span> *makes the current realization *<span  class="math">\(\tilde{X}\)</span> <em>most likely</em>. In order to do this, observe that the joint-distribution of the observed data is given by
<span  class="math">\(f(\theta;x_1=X_1,x_2=X_2,\cdots,x_n=X_n)=\prod_{i=1}^{n} f(x_i;\theta):=L(\theta)\)</span>
This is the likelihood function. An intuitive estimate to the parameter would now be the value that makes the data points observed <em>most likely</em>; simply given by
<span  class="math">\(\hat{\theta}_{\mathrm{MLE}}=\underset{\theta}{\mathrm{argmax\,}} L(\theta) = \mathrm{arg}\{\frac{dL}{d\theta}=0\}\)</span>
Notice that this requires the likelihood function to be differentiable in order to be maximized; and often a log-likelihood is maximized instead of the likelihood function instead.
The Maximum likelihood estimator is <a href="https://en.wikipedia.org/wiki/Consistent_estimator">consistent</a>, and often <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">unbiased</a>. Then the obvious question is:</p>

<p>##Why do MoM? (or some other estimator)
For this, consider a family of Gamma distribution with parameters <span  class="math">\(\theta_1=\alpha, \theta_2=\beta\)</span> such that <span  class="math">\(\theta_1,\theta_2>0\)</span>, be given by:
<span  class="math">\(f(x;\theta_1,\theta_2)=\frac{1}{\Gamma(\theta_1)\theta_2^{\theta_1}}x^{\theta_1-1}e^{-\frac{x}{\theta_2}}\)</span>
For a particular sample of data <span  class="math">\(\tilde{X}\)</span> from this family, we get the likelihood function as:
<span  class="math">\(L(\theta_1,\theta_2;x_1,\cdots,x_n)=[\frac{1}{\Gamma(\theta_1)\theta_2^{\theta_1}}]^n(x_1 x_2\cdots x_n)^{\theta_1-1}\mathrm{exp}(-\sum_{i=1}^{n} x_i/\theta_2)\)</span>
Not so easy to find the MLE now, is it!
<em>The gamma function in the likelihood makes it hard to find the MLE in a closed form in this example, and in general.</em></p>

<p>##MoM
MoM is another intuitive way to proceed in such problems where MLE either does not exist, or is hard to calculate.
The underlying principle of an MLE, which is very easy to follow, is:
Equate <span  class="math">\(\frac{1}{n}\sum_{i=1}^{n}X_i^k\)</span> to the expectation <span  class="math">\(\mathbb{E}[x^k]\)</span> for <span  class="math">\(k = 1,2,\cdots\)</span> until you have enough equations to solve for parameters <span  class="math">\(\theta\)</span>.
Pretty easy, right!
Then why do we need other estimators?</p>

<p>##Why do MLE?
Suppose, in the very same example of the gamma-family, you need to find the parameters <span  class="math">\(\theta_1\)</span> and let <span  class="math">\(\theta_2=\beta\)</span> be given, for simplicity.
The likelihood of the observed sample is:
<span  class="math">\(L(\theta_1;x_1,\cdots,x_n)=[\frac{1}{\Gamma(\theta_1)\beta^{\theta_1}}]^n(x_1 x_2\cdots x_n)^{\theta_1-1}\mathrm{exp}(-\sum_{i=1}^{n} x_i/\beta)\)</span>
<span  class="math">\(= ([\frac{1}{\Gamma(\theta_1)\beta^{\theta_1}}]^n \mathrm{exp}(-\sum_{i=1}^{n} x_i/\beta)) \cdot (\mathrm{exp}\{(\theta_1-1)\sum^n_1 \mathrm{log}(x_i)\})\)</span>
Let me digress for a moment and recall the reader’s attention to the concept of a <a href="https://en.wikipedia.org/wiki/Sufficient_statistic">sufficient statistic</a>. In a broad manner, the sufficient statistic encompasses <em>all the information</em> that could possibly be conveyed about the parameter to be estimated <span  class="math">\(\theta\in\Omega\)</span> by the observed data <span  class="math">\(\tilde{X}\)</span>. Therefore, it is always great to use this information in our estimate <span  class="math">\(\hat{\theta}\)</span>.
By using Factorization theorem on the likelihood expression above, the sufficient statistic for $\theta_1$ is given by some <span  class="math">\(Y=u(\tilde{X})=\sum^n_1\mathrm{log}\,x_i\)</span>.
Note that if you use MoM to estimate the parameters here, the estimate <span  class="math">\(\hat{\theta}\)</span> would *not be a function of *<span  class="math">\(Y\)</span>; it would always be a function of the sample moments <span  class="math">\(\frac{1}{n}\sum_{i=1}^{n}X_i^k\)</span> (sample mean, sample variance, etc.).
We can conclusively say from this, that</p>

<p><strong>MoM estimate need not be a function of the sufficient statistic; an MLE, if it exists, is ALWAYS a function of the sufficient statistic</strong>*.
This is where MLE shines, and this is why it is used; if it is useful to a problem!
However, MoM estimates are good starting point for numerically complex point estimation problems and can iteratively lead to a good estimate and are consistent estimates.</p>

<p>##Why MAP?
Coming back to the definition of the point estimation problem, suppose in addition to <span  class="math">\(\theta\in\Omega\)</span>, we have some more knowledge about where it could be in <span  class="math">\(\Omega\)</span>; this is usually in the form of some *prior knowledge about *<span  class="math">\(\theta\sim g(\theta|\theta\in\Omega)\)</span>; hence called the prior.
The MAP estimate is a way to include this knowledge in our estimation as
<span  class="math">\(\hat{\theta}_{\mathrm{MAP}}=\underset{\theta}{\mathrm{argmax\,}} f(x;\theta)g(\theta)\)</span>
This is just using the fact that *using more information to find <em><span  class="math">\(\hat{\theta}\)</span></em> wouldn’t hurt!*</p>

<p>Now that we know about what is going on, I would answer the questions asked here.</p>

<blockquote>
<p>What is the difference between MAP, MLE, MoM?
I have already answered in terms of the differences. So I would rather answer “When to use MoM, MLE or MAP?” based on my observations:</p>

<ol>
<li>Do you have a prior knowledge about $\theta$?</li>
<li>If yes

<ol>
<li>Is your prior uniform?

<ol>
<li>No → <strong>MAP estimator</strong></li>
<li>Yes; Does a closed form MLE exist?
Yes → **MLE estimator
** No → <strong>MoM estimator</strong></li>
</ol></li>
</ol></li>
<li>If No, go to 2. (ii) ; use MLE/MoM
This is only to help choose among the given 3 methods.
Are there instances where the MOM and MLE are exactly the same?
Yes.
If your MoM estimate is a function of the sufficient statistic, the two estimates <em>could</em> be the same.
If MoM estimate is <em>not</em> a function of the sufficient statistic, the two are <em>definitely</em> different estimates.
Interestingly enough, as we observed, MoM estimates need not be a function of the sufficient statistic and the estimate need not even lie in the region <span  class="math">\(\Omega\)</span>!
Further, the MAP estimate would coincide with the MLE estimate iff <span  class="math">\(g(\theta)=\mathrm{Uniform}(\Omega)\)</span>.</li>
</ol>
</blockquote>

<p>I have been superficial in addressing the question so that it is easily understandable. Most of the stuff comes from Introduction to Mathematical Statistics, 7th Edition (<a href="https://www.pearsonhighered.com/program/Hogg-Introduction-to-Mathematical-Statistics-7th-Edition/PGM49624.html">https://www.pearsonhighered.com/program/Hogg-Introduction-to-Mathematical-Statistics-7th-Edition/PGM49624.html</a>), which is a great introductory (and almost standard) text.</p>

		</div><div id="disqus_thread"></div>
<script type="text/javascript">
	(function () {
		
		
		if (window.location.hostname == "localhost")
			return;

		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		var disqus_shortname = 'https-omanshuthapliyal-github-io-blog';
		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
		Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
	
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-144848923-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<script>feather.replace()</script>
</body>
</html>
